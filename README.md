A highly motivated data analyst with expertise in interpreting, analyzing and reconciliation of data obtained from a variety of sources.
-----------------------------------------------------------------------------------------------------------------------------------------
Education	
-----------------------------------------------------------------------------------------------------------------------------------------
Master of Science in Data Science Engineering	May 2023
Gannon University, Erie, Pennsylvania	
Bachelor of Engineering in Electronics and Communication Engineering	May 2019
BML Munjal University, Gurgaon, India	
----------------------------------------------------------------------------------------------------------------------------------------
Technical Proficiencies
----------------------------------------------------------------------------------------------------------------------------------------
 Languages:	SQL | Python | Java | R | C++ | HTML | Perl | JavaScript | Shell Scripting | Spark | Scala
 Databases:	MySQL | Hive | MS SQL Server | Hadoop | Oracle | MongoDB | Azure Cosmos DB
 Operating Systems	Windows | MacOS | Linux | UNIX
 ML Libraries:	TensorFlow | pandas | NumPy | Scikit learn | seaborn
 Cloud Computing:	Amazon Web Services (AWS) | RDS | S3 | EC2 | Snowflake | Microsoft Azure | Databricks 
 Tools:	Git | Tableau | Weka | Jira | MS Office | Analytical Solver | RStudio | Jupyter Notebook | Power BI | Zeppelin | Visual Studio
-----------------------------------------------------------------------------------------------------------------------------------------
Professional Experience
----------------------------------------------------------------------------------------------------------------------------------------
Research Intern, Gannon-University, Erie  | 06/2022 – 05/2023
---------------------------------------------------------------------------------------------------------------------------------------
Employed Tableau for data analysis and visualization on a dataset of 11,430 URLs with 87 features.
Developed phishing detection system using Random Forest, XGBoost, MLP, Gaussian Naive Bayes.
Achieved F1 scores of 0.83 and 0.96 on separate feature sets using train-test split.

Data Analyst, Cedar Infotech, Hyderabad, India  | 01/2020 – 06/2021 
-----------------------------------------------------------------------------------------------------------------------------------------
Utilized SQL and data visualization to analyze and report client data requests.
Created Data Visualization dashboards using Tableau.
Validated data transformations and performed end-to-end data validation for ETL and BI systems.
Developed test strategies, plans, and executed test cases for ETL and BI systems.

Application Developer, SR Akshay Academy, Hyderabad, India  | 01/2019 – 12/2019 
-----------------------------------------------------------------------------------------------------------------------------------------
Developed a Predictive model to reduce student dropout rates, improving company revenue.
Conducted data cleaning and exploratory analysis using Power BI and complex SQL queries.
Created packages for Extract, Transform, Load (ETL) operations using SQL Server Integration Services (SSIS).

-----------------------------------------------------------------------------------------------------------------------------------------
Data Science & Software Academic Projects
-----------------------------------------------------------------------------------------------------------------------------------------

1.Phishing Detection System with Advanced Machine Learning
----------------------------------------------------------------------------------------------------------------------------------------
![phishingdetection](https://github.com/sarangu001/myportfolio/assets/91167302/6a7eaadc-fd20-4189-bdbc-177187d7a1b6)




I have developed an advanced phishing detection system leveraging cutting-edge machine learning algorithms including Random Forest, XGBoost, Multi-Layer Perceptron, and Gaussian Naive Bayes. This system was designed to combat the ever-evolving threat landscape of online phishing attacks. It utilized a dataset comprising 11,430 URLs, each described by a rich set of 87 diverse features.
My approach for phishing detection was comprehensive, integrating syntax-based, content-based, and external querying services to maximize accuracy. To ensure the most effective feature extraction, I harnessed the power of Tableau, a powerful data visualization tool. I also conducted extensive experiments with six different datasets, achieving outstanding accuracy rates of 87.7% and 86.6% using the Random Forest and XGBoost algorithms, respectively. These impressive results were obtained when utilizing a feature set consisting of 16 hybrid features.
One of the unique aspects of this project involved the use of mathematical analysis with 3-dimensional features to effectively distinguish phishing URLs from legitimate ones. Among the various datasets tested, the dataset containing all 87 features achieved the highest accuracy and AUC-ROC score. Notably, a dataset with 49 syntax-based features also yielded strong results. Interestingly, content-based features exhibited lower accuracy compared to other feature sets. However, feature selection emerged as a critical factor in model performance. A hybrid dataset containing just six selected features demonstrated performance on par with the one featuring all 87 features.

The training model I designed incorporated not only traditional labeling but also phishing and legitimate URL counts as labels. This unconventional approach proved to be highly effective in enhancing the system's ability to discern phishing attempts from legitimate URLs. The use of 3-dimensional features further pushed the boundaries of detection accuracy.

My phishing detection system represents a significant advancement in online security. By combining state-of-the-art machine learning algorithms, a rich feature set, and innovative techniques for feature selection and extraction, I achieved remarkable accuracy rates. This project underscores my commitment to addressing real-world challenges with creative and effective solutions in the field of machine learning and cybersecurity.

--------------------------------------------------------------------------------------------------------------------------------------
2.Decision and risk analysis of wildfires in California: Analyzed data to predict and minimize wildfire risks in real time scenarios.
--------------------------------------------------------------------------------------------------------------------------------------
![wildfire](https://github.com/sarangu001/myportfolio/assets/91167302/47fbc1dd-ffb4-401d-a5bc-004676675520)


The primary objective of this project was to develop a robust and data-driven system capable of predicting and mitigating wildfire risks in real-time scenarios within the California region. Wildfires in California pose significant threats to lives, property, and the environment. This project aimed to harness advanced data analytics and decision-making techniques to enhance wildfire risk management and response strategies.
The project initiated with an extensive data analysis phase. We collected and curated diverse datasets, including historical wildfire records, weather patterns, vegetation indices, and geographical information. These datasets were meticulously cleaned and processed to ensure data quality.

We employed a range of machine learning models, such as Random Forest, Support Vector Machines, and Gradient Boosting, to build predictive models for wildfire risk. These models were trained on historical data to capture complex relationships between various factors contributing to wildfires.
One of the key technical achievements of this project was the development of real-time prediction capabilities. We integrated live data streams from weather stations, satellite imagery, and ground sensors into our models, enabling us to provide up-to-the-minute wildfire risk assessments. This feature was critical for timely decision-making by relevant authorities.

In addition to predicting wildfire risks, we also designed risk mitigation strategies. By utilizing advanced optimization techniques, we developed dynamic evacuation plans and resource allocation strategies. These plans considered factors like population density, road networks, and fire progression models to ensure the most effective response in high-risk areas.

Given the inherent uncertainty in wildfire behavior, we conducted comprehensive uncertainty analysis. Monte Carlo simulations and sensitivity analyses were employed to assess the robustness of our predictions and evaluate the impact of various input parameters on the outcomes.

To aid decision-makers, we created geospatial visualizations using Geographic Information System (GIS) tools. These visualizations provided a clear and actionable representation of wildfire risks, allowing for better resource allocation and evacuation planning.

I achieved a significant enhancement in wildfire risk prediction accuracy and real-time monitoring capabilities. The system has been tested and validated using historical wildfire events and has shown promising results in real-world scenarios. By providing timely and data-driven insights, it equips emergency responders and policymakers with valuable tools to mitigate the devastating impact of wildfires in California.

This project exemplifies my expertise in data analytics, machine learning, and risk assessment, and its application to critical real-world challenges. It underscores the potential for technology to make a substantial positive impact on disaster management and public safety.

-----------------------------------------------------------------------------------------------------------------------------------------
3.Language Translation Pipeline: Designed a translator using Deep Learning methods to translate French and English texts.
-----------------------------------------------------------------------------------------------------------------------------------------
![languagetranslation](https://github.com/sarangu001/myportfolio/assets/91167302/4dfc3825-b217-43e3-9422-41379b45a790)



In this project, I undertook the development of a sophisticated language translation pipeline, harnessing the power of deep learning techniques to facilitate seamless translation between French and English languages. This endeavor represents a culmination of cutting-edge natural language processing (NLP) methodologies and data-driven language modeling.

The project commenced with an extensive data collection effort, acquiring substantial bilingual text corpora in both French and English. To ensure data quality and consistency, rigorous preprocessing steps were applied, including tokenization, normalization, and alignment of parallel text sentences.

At the core of this translation pipeline were sequence-to-sequence models, a category of deep neural networks specially designed for tasks like machine translation. I implemented state-of-the-art architectures such as the Transformer model, which has demonstrated remarkable performance in NLP tasks.

Training these deep learning models was a computationally intensive process. It involved optimizing various hyperparameters, including model architecture, learning rates, batch sizes, and the incorporation of techniques like dropout and layer normalization to enhance model convergence and translation quality.

To improve the model's ability to capture nuanced language nuances and translation subtleties, I leveraged multilingual embeddings and attention mechanisms. These components allowed the model to learn contextual information from input sequences and generate coherent and contextually appropriate translations.

Comprehensive evaluation of the translation pipeline was conducted using a variety of performance metrics, including BLEU (Bilingual Evaluation Understudy), TER (Translation Edit Rate), and METEOR (Metric for Evaluation of Translation with Explicit ORdering). These metrics provided insights into the system's translation accuracy and fluency.

In order to make the translation pipeline accessible and user-friendly, I developed a web-based user interface. This interface allowed users to input text in either language and receive near-instantaneous translations, making it a practical tool for individuals and organizations requiring accurate French-English translation services.

The architecture of the translation pipeline was designed with scalability in mind, making it adaptable for other language pairs and extensions to accommodate domain-specific translation needs. Additionally, future enhancements may include integrating speech-to-text and text-to-speech components for comprehensive language services.

This Language Translation Pipeline project represents a significant achievement in the realm of deep learning and natural language processing. It demonstrates my proficiency in handling complex NLP tasks, model development, and user interface design. The practical applications of this technology are far-reaching, encompassing industries such as localization, content translation, and international communication.

----------------------------------------------------------------------------------------------------------------------------------------
4.Exploiting Race Condition Vulnerability in a Virtual Machine: Explored vulnerabilities in virtual machines using C programming and symbolic link techniques.
-----------------------------------------------------------------------------------------------------------------------------------------
![virtual](https://github.com/sarangu001/myportfolio/assets/91167302/195264d4-c16e-4d62-9f40-22722a1d863c)


In this technically intricate project, I embarked on a journey to delve deep into the world of virtual machines (VMs) to uncover vulnerabilities related to race conditions. Leveraging my expertise in C programming and symbolic link techniques, I conducted a comprehensive exploration of these vulnerabilities, shedding light on critical security concerns within virtualized environments.

Virtual machines are widely used for their isolation and security benefits. However, as with any technology, they are not immune to security flaws. Race condition vulnerabilities, a type of concurrency issue, can jeopardize the integrity and confidentiality of data within VMs. My project aimed to identify, understand, and exploit these vulnerabilities.

The core of this project revolved around extensive C programming. I meticulously crafted and manipulated code to create controlled race conditions within the virtual machine environment. This involved carefully orchestrating multiple threads or processes to interact with shared resources in unpredictable ways, simulating real-world scenarios where race conditions might occur.
To further enhance the complexity and real-world relevance of my experiments, I applied symbolic link techniques. Symbolic links, also known as symlinks, are powerful tools that allow for the manipulation of file system paths. By skillfully employing symlinks, I escalated the potential impact of race condition vulnerabilities, demonstrating how they could be leveraged for unauthorized access or data manipulation.

As part of this project, I extensively documented the security implications of the race condition vulnerabilities discovered. My analysis delved into the potential damage that could be inflicted if these vulnerabilities were to be exploited maliciously. Furthermore, I proposed and discussed mitigation strategies, emphasizing the importance of secure coding practices and careful resource management within virtualized environments.

To validate the presence of race condition vulnerabilities and their potential exploitation, I meticulously designed and executed a series of controlled experiments. These experiments were conducted in a controlled VM environment, ensuring the safety and isolation of the testing process while highlighting the real-world implications of the vulnerabilities.
It is important to note that all aspects of this project were conducted within a strictly ethical framework. My intent was solely to identify vulnerabilities, assess their impact, and propose countermeasures to strengthen the security of virtual machines.By delving into the intricate world of race condition vulnerabilities within virtual machines, this project not only highlights my expertise in low-level programming but also contributes to the broader field of cybersecurity. It underscores the need for continuous vigilance and proactive measures to safeguard virtualized environments against emerging threats.

-----------------------------------------------------------------------------------------------------------------------------------------
5.Dynamic Netflix Website Prototype with Genre-based Selection: Developed an interactive Netflix prototype using ReactJS and APIs for dynamic movie selection.
-----------------------------------------------------------------------------------------------------------------------------------------
![netflix](https://github.com/sarangu001/myportfolio/assets/91167302/14a0b745-0c16-4db5-82d5-f6227b1ce67e)



 I designed and implemented a highly dynamic and user-centric platform for selecting movies based on genre preferences. This project showcases my proficiency in front-end development, API integration, and user interface design.

ReactJS: The project's foundation rested on ReactJS, a popular JavaScript library for building user interfaces. React's component-based architecture allowed for efficient management of complex UI elements, ensuring a smooth and responsive user experience.
API Integration: To provide a vast selection of movies and genres, I integrated third-party APIs into the prototype. These APIs served as the primary data source, supplying real-time movie information, including titles, descriptions, genres, and ratings.A hallmark of this project was its focus on delivering an engaging and immersive user experience. The prototype offered several features to enhance user interaction:

Genre-based Selection: Users could browse movies by genre, tailoring their viewing choices to their preferences. This feature was implemented using API endpoints that allowed for genre filtering.

Dynamic Content Loading: Leveraging asynchronous requests and React's virtual DOM, the prototype seamlessly loaded movie data without requiring full page reloads, creating a fluid browsing experience.

Movie Details: Users could access detailed information about each movie, including synopses, cast, and viewer ratings. This information was dynamically fetched from the APIs and presented in an organized and visually appealing manner.

Responsive Design: The prototype was designed to be responsive, ensuring optimal viewing experiences on a wide range of devices, from desktops to smartphones.

To make the prototype feel like a genuine streaming service, I implemented realistic features:
User Authentication: A basic user authentication system allowed users to create accounts, log in, and personalize their movie recommendations.
Recommendation Engine: I designed a rudimentary recommendation engine that suggested movies based on user viewing history and genre preferences, adding an element of personalization to the platform.

Caching and Data Management: To reduce unnecessary API calls, I implemented client-side caching and optimized data retrieval mechanisms.
Lazy Loading: Images and content were loaded lazily, ensuring that only the necessary assets were fetched when a user interacted with the platform.

The architecture of the prototype was designed with scalability in mind, making it adaptable for future enhancements. Features like user reviews, watchlists, and advanced recommendation algorithms could be seamlessly integrated into the platform.

This project underscores my commitment to creating user-centric web applications that offer both functionality and interactivity. It also highlights my skills in leveraging modern web development technologies to deliver engaging user experiences.






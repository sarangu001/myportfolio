A highly motivated data science professional with expertise in interpreting, analyzing and reconciliation of data obtained from a variety of sources.
-----------------------------------------------------------------------------------------------------------------------------------------

Education :
--------------

Master of Science in Data Science Engineering  
Gannon University, Erie, Pennsylvania 	
| May 2023

Bachelor of Engineering in Electronics and Communication Engineering
BML Munjal University, Gurgaon, India	
| May 2019


Technical Proficiencies:
-----------------------------------------------------------------------------------------------------------------------------------------

 Languages:	
 SQL | Python | Java | R | C++ | HTML | Perl | JavaScript | Shell Scripting | Spark | Scala
 
 Databases:	
 MySQL | Hive | MS SQL Server | Hadoop | Oracle | MongoDB | Azure Cosmos DB
 
 Operating Systems:
 Windows | MacOS | Linux | UNIX
 
 ML Libraries:
 TensorFlow | pandas | NumPy | Scikit learn | seaborn
 
 Cloud Computing:
 Amazon Web Services (AWS) | RDS | S3 | EC2 | Snowflake | Microsoft Azure | Databricks 
 
 Tools:
 Git | Tableau | Weka | Jira | MS Office | Analytical Solver | RStudio | Jupyter Notebook | Power BI | Zeppelin | Visual Studio
 

Professional Experience:
-------------------------


Research Intern, Gannon-University, Erie  | 06/2022 – 05/2023
----------------------------------------------------------------
Employed Tableau for data analysis and visualization on a dataset of 11,430 URLs with 87 features.
Developed phishing detection system using Random Forest, XGBoost, MLP, Gaussian Naive Bayes.
Achieved F1 scores of 0.83 and 0.96 on separate feature sets using train-test split.


Global Student Host, Gannon University, Erie  | 08/2022 – 01/2023
-----------------------------------------------------------------
Efficiently handled financial transactions and student accounts at Gannon University's Cashier’s Office, developing adeptness in CRM systems and enhancing data management and financial operation skills.
Collaborated with university staff and faculty to optimize the student experience.
Organized and managed cultural exchange events. Welcomed and assisted international students in their transition to Gannon University, fostering an inclusive environment.
Utilized emotional intelligence to address student concerns, acting as a reliable point of contact for their needs.


Data Analyst, Cedar Infotech, Hyderabad, India  | 01/2020 – 06/2021
-------------------------------------------------------------------
Utilized SQL and data visualization to analyze and report client data requests.
Created Data Visualization dashboards using Tableau.
Validated data transformations and performed end-to-end data validation for ETL and BI systems.
Developed test strategies, plans, and executed test cases for ETL and BI systems.

Application Developer, SR Akshay Academy, Hyderabad, India  | 01/2019 – 12/2019 
-------------------------------------------------------------------------------
Developed a Predictive model to reduce student dropout rates, improving company revenue.
Conducted data cleaning and exploratory analysis using Power BI and complex SQL queries.
Created packages for Extract, Transform, Load (ETL) operations using SQL Server Integration Services (SSIS).

Machine Learning Intern, Byjus,Delhi, India  | 08/2018 – 12/2018 
-----------------------------------------------------------------
Utilized Spark, Scala, Hadoop, HQL. VQL, oozie, AWS, Python, Mmlib, CUDA, Kafka, Kinesis, Spark Streaming, Edward, Cassandra, HBase, Data Lake, TensorFlow, Redshift, MongoDB including classifications, regressions, dimensionally reduction to increase user lifetime by 45% and triple user conversations.	Analyzed data from Google Analytics, Ad Words. Evaluated models by using Cross Validation, Log loss function.




Data Science & Software Academic Projects
------------------------------------------

1.Phishing Detection System with Advanced Machine Learning
----------------------------------------------------------
![phishingdetection](https://github.com/sarangu001/myportfolio/assets/91167302/6a7eaadc-fd20-4189-bdbc-177187d7a1b6)

I developed an advanced phishing detection system using cutting-edge machine learning algorithms like Random Forest, XGBoost, Multi-Layer Perceptron, and Gaussian Naive Bayes. It analyzed 11,430 URLs with 87 diverse features, combining syntax-based, content-based, and external querying services. Feature extraction was optimized with Tableau data visualization.

Through experiments with six datasets, I achieved exceptional accuracy rates of 87.7% (Random Forest) and 86.6% (XGBoost) using a 16-feature hybrid set. Mathematical analysis with 3D features effectively separated phishing from legitimate URLs. The full 87-feature dataset and a 49-feature syntax-based set performed best.

Unconventionally, I used phishing and legitimate URL counts as labels during training, enhancing detection accuracy. My system represents a significant advancement in online security, demonstrating my commitment to addressing real-world challenges in machine learning and cybersecurity.





2.Decision and risk analysis of wildfires in California: Analyzed data to predict and minimize wildfire risks in real time scenarios.
--------------------------------------------------------------------------------------------------------------------------------------
![wildfire](https://github.com/sarangu001/myportfolio/assets/91167302/47fbc1dd-ffb4-401d-a5bc-004676675520)


The main goal of this project was to create an advanced system for predicting and managing wildfire risks in real-time in California. Wildfires in the region are a major threat to people, property, and the environment. Our approach involved using data analysis and machine learning to improve wildfire risk management.

We began by gathering and cleaning various datasets, including historical wildfire data, weather information, vegetation data, and geographical details. These datasets formed the basis for our predictive models.

We used machine learning models like Random Forest, Support Vector Machines, and Gradient Boosting to build these models. They were trained on historical data to understand the complex factors that contribute to wildfires.

A key achievement was our ability to incorporate real-time data from sources like weather stations, satellites, and ground sensors into our models. This allowed us to provide minute-by-minute wildfire risk assessments, aiding timely decision-making.

We didn't just predict wildfire risks; we also designed strategies to mitigate them. We used advanced optimization techniques to create evacuation plans and allocate resources effectively, considering factors like population density and road networks.

Given the uncertainty in wildfire behavior, we conducted extensive uncertainty analysis using Monte Carlo simulations and sensitivity analyses to test our predictions' robustness.

To assist decision-makers, we generated geospatial visualizations using Geographic Information System (GIS) tools. These visualizations offered clear representations of wildfire risks for better resource allocation and evacuation planning.

Our system significantly improved wildfire risk prediction accuracy and real-time monitoring. It has been tested with historical events and performed well in real-world scenarios. This project demonstrates how data analytics, machine learning, and risk assessment can address critical challenges, especially in disaster management and public safety.







3.Language Translation Pipeline: Designed a translator using Deep Learning methods to translate French and English texts.
--------------------------------------------------------------------------------------------------------------------------
![languagetranslation](https://github.com/sarangu001/myportfolio/assets/91167302/4dfc3825-b217-43e3-9422-41379b45a790)



In this project, I undertook the development of a sophisticated language translation pipeline, harnessing the power of deep learning techniques to facilitate seamless translation between French and English languages. This endeavor represents a culmination of cutting-edge natural language processing (NLP) methodologies and data-driven language modeling.

The project commenced with an extensive data collection effort, acquiring substantial bilingual text corpora in both French and English. To ensure data quality and consistency, rigorous preprocessing steps were applied, including tokenization, normalization, and alignment of parallel text sentences.

At the core of this translation pipeline were sequence-to-sequence models, a category of deep neural networks specially designed for tasks like machine translation. I implemented state-of-the-art architectures such as the Transformer model, which has demonstrated remarkable performance in NLP tasks.

Training these deep learning models was a computationally intensive process. It involved optimizing various hyperparameters, including model architecture, learning rates, batch sizes, and the incorporation of techniques like dropout and layer normalization to enhance model convergence and translation quality.

To improve the model's ability to capture nuanced language nuances and translation subtleties, I leveraged multilingual embeddings and attention mechanisms. These components allowed the model to learn contextual information from input sequences and generate coherent and contextually appropriate translations.

Comprehensive evaluation of the translation pipeline was conducted using a variety of performance metrics, including BLEU (Bilingual Evaluation Understudy), TER (Translation Edit Rate), and METEOR (Metric for Evaluation of Translation with Explicit ORdering). These metrics provided insights into the system's translation accuracy and fluency.

In order to make the translation pipeline accessible and user-friendly, I developed a web-based user interface. This interface allowed users to input text in either language and receive near-instantaneous translations, making it a practical tool for individuals and organizations requiring accurate French-English translation services.

The architecture of the translation pipeline was designed with scalability in mind, making it adaptable for other language pairs and extensions to accommodate domain-specific translation needs. Additionally, future enhancements may include integrating speech-to-text and text-to-speech components for comprehensive language services.

This Language Translation Pipeline project represents a significant achievement in the realm of deep learning and natural language processing. It demonstrates my proficiency in handling complex NLP tasks, model development, and user interface design. The practical applications of this technology are far-reaching, encompassing industries such as localization, content translation, and international communication.




4.Exploiting Race Condition Vulnerability in a Virtual Machine: Explored vulnerabilities in virtual machines using C programming and symbolic link techniques.
--------------------------------------------------------------------------------------------------------------------------------------------------------------
![virtual](https://github.com/sarangu001/myportfolio/assets/91167302/195264d4-c16e-4d62-9f40-22722a1d863c)


In this technically intricate project, I embarked on a journey to delve deep into the world of virtual machines (VMs) to uncover vulnerabilities related to race conditions. Leveraging my expertise in C programming and symbolic link techniques, I conducted a comprehensive exploration of these vulnerabilities, shedding light on critical security concerns within virtualized environments.

Virtual machines are widely used for their isolation and security benefits. However, as with any technology, they are not immune to security flaws. Race condition vulnerabilities, a type of concurrency issue, can jeopardize the integrity and confidentiality of data within VMs. My project aimed to identify, understand, and exploit these vulnerabilities.

The core of this project revolved around extensive C programming. I meticulously crafted and manipulated code to create controlled race conditions within the virtual machine environment. This involved carefully orchestrating multiple threads or processes to interact with shared resources in unpredictable ways, simulating real-world scenarios where race conditions might occur.
To further enhance the complexity and real-world relevance of my experiments, I applied symbolic link techniques. Symbolic links, also known as symlinks, are powerful tools that allow for the manipulation of file system paths. By skillfully employing symlinks, I escalated the potential impact of race condition vulnerabilities, demonstrating how they could be leveraged for unauthorized access or data manipulation.

As part of this project, I extensively documented the security implications of the race condition vulnerabilities discovered. My analysis delved into the potential damage that could be inflicted if these vulnerabilities were to be exploited maliciously. Furthermore, I proposed and discussed mitigation strategies, emphasizing the importance of secure coding practices and careful resource management within virtualized environments.

To validate the presence of race condition vulnerabilities and their potential exploitation, I meticulously designed and executed a series of controlled experiments. These experiments were conducted in a controlled VM environment, ensuring the safety and isolation of the testing process while highlighting the real-world implications of the vulnerabilities.
It is important to note that all aspects of this project were conducted within a strictly ethical framework. My intent was solely to identify vulnerabilities, assess their impact, and propose countermeasures to strengthen the security of virtual machines.By delving into the intricate world of race condition vulnerabilities within virtual machines, this project not only highlights my expertise in low-level programming but also contributes to the broader field of cybersecurity. It underscores the need for continuous vigilance and proactive measures to safeguard virtualized environments against emerging threats.



5.Dynamic Netflix Website Prototype with Genre-based Selection: Developed an interactive Netflix prototype using ReactJS and APIs for dynamic movie selection.
-----------------------------------------------------------------------------------------------------------------------------------------
![netflix](https://github.com/sarangu001/myportfolio/assets/91167302/14a0b745-0c16-4db5-82d5-f6227b1ce67e)



 I designed and implemented a highly dynamic and user-centric platform for selecting movies based on genre preferences. This project showcases my proficiency in front-end development, API integration, and user interface design.

ReactJS: The project's foundation rested on ReactJS, a popular JavaScript library for building user interfaces. React's component-based architecture allowed for efficient management of complex UI elements, ensuring a smooth and responsive user experience.
API Integration: To provide a vast selection of movies and genres, I integrated third-party APIs into the prototype. These APIs served as the primary data source, supplying real-time movie information, including titles, descriptions, genres, and ratings.A hallmark of this project was its focus on delivering an engaging and immersive user experience. The prototype offered several features to enhance user interaction:

Genre-based Selection: Users could browse movies by genre, tailoring their viewing choices to their preferences. This feature was implemented using API endpoints that allowed for genre filtering.

Dynamic Content Loading: Leveraging asynchronous requests and React's virtual DOM, the prototype seamlessly loaded movie data without requiring full page reloads, creating a fluid browsing experience.

Movie Details: Users could access detailed information about each movie, including synopses, cast, and viewer ratings. This information was dynamically fetched from the APIs and presented in an organized and visually appealing manner.

Responsive Design: The prototype was designed to be responsive, ensuring optimal viewing experiences on a wide range of devices, from desktops to smartphones.

To make the prototype feel like a genuine streaming service, I implemented realistic features:
User Authentication: A basic user authentication system allowed users to create accounts, log in, and personalize their movie recommendations.
Recommendation Engine: I designed a rudimentary recommendation engine that suggested movies based on user viewing history and genre preferences, adding an element of personalization to the platform.

Caching and Data Management: To reduce unnecessary API calls, I implemented client-side caching and optimized data retrieval mechanisms.
Lazy Loading: Images and content were loaded lazily, ensuring that only the necessary assets were fetched when a user interacted with the platform.

The architecture of the prototype was designed with scalability in mind, making it adaptable for future enhancements. Features like user reviews, watchlists, and advanced recommendation algorithms could be seamlessly integrated into the platform.

This project underscores my commitment to creating user-centric web applications that offer both functionality and interactivity. It also highlights my skills in leveraging modern web development technologies to deliver engaging user experiences.





